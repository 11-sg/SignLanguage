{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import pickle  # To save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Holistic Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_PATH = r'C:\\Users\\SAHIL GUPTA\\Downloads\\GestureDataTemp\\Gesture Image Pre-Processed Data'\n",
    "actions = sorted(os.listdir(DATASET_PATH))  # List of gesture classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '_']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract hand keypoints + wrist landmarks\n",
    "def extract_keypoints(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image)\n",
    "\n",
    "    # Extract landmarks\n",
    "    left_hand = results.left_hand_landmarks.landmark if results.left_hand_landmarks else []\n",
    "    right_hand = results.right_hand_landmarks.landmark if results.right_hand_landmarks else []\n",
    "\n",
    "    def get_landmark_array(landmarks):\n",
    "        return np.array([[lm.x, lm.y, lm.z] for lm in landmarks]) if landmarks else np.zeros((21, 3))\n",
    "\n",
    "    # Full hand keypoints\n",
    "    left_hand_keypoints = get_landmark_array(left_hand)\n",
    "    right_hand_keypoints = get_landmark_array(right_hand)\n",
    "\n",
    "    # Extract wrist landmarks (landmark 0) separately if available\n",
    "    left_wrist = left_hand_keypoints[0] if left_hand else np.zeros(3)\n",
    "    right_wrist = right_hand_keypoints[0] if right_hand else np.zeros(3)\n",
    "\n",
    "    # Concatenate all: left_hand + right_hand + left_wrist + right_wrist\n",
    "    return np.concatenate([\n",
    "        left_hand_keypoints.flatten(),\n",
    "        right_hand_keypoints.flatten(),\n",
    "        left_wrist,  # (x, y, z)\n",
    "        right_wrist  # (x, y, z)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"1.jpg\")  # load your image\n",
    "keypoints = extract_keypoints(image)\n",
    "print(keypoints.shape)\n",
    "print(keypoints)  # this prints the 132 keypoint values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0:   0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0: 100%|██████████| 1500/1500 [00:53<00:00, 28.25it/s]\n",
      "Processing 1: 100%|██████████| 1500/1500 [00:47<00:00, 31.33it/s]\n",
      "Processing 2: 100%|██████████| 1500/1500 [00:48<00:00, 31.15it/s]\n",
      "Processing 3: 100%|██████████| 1500/1500 [00:47<00:00, 31.25it/s]\n",
      "Processing 4: 100%|██████████| 1500/1500 [00:47<00:00, 31.31it/s]\n",
      "Processing 5: 100%|██████████| 1500/1500 [00:49<00:00, 30.50it/s]\n",
      "Processing 6: 100%|██████████| 1500/1500 [00:48<00:00, 30.75it/s]\n",
      "Processing 7: 100%|██████████| 1500/1500 [00:49<00:00, 30.31it/s]\n",
      "Processing 8: 100%|██████████| 1500/1500 [00:49<00:00, 30.26it/s]\n",
      "Processing 9: 100%|██████████| 1500/1500 [00:48<00:00, 30.85it/s]\n",
      "Processing A: 100%|██████████| 1500/1500 [05:23<00:00,  4.64it/s] \n",
      "Processing B: 100%|██████████| 1500/1500 [00:53<00:00, 27.97it/s]\n",
      "Processing C: 100%|██████████| 1500/1500 [00:53<00:00, 27.92it/s]\n",
      "Processing D: 100%|██████████| 1500/1500 [00:53<00:00, 27.95it/s]\n",
      "Processing E: 100%|██████████| 1500/1500 [00:49<00:00, 30.55it/s]\n",
      "Processing F: 100%|██████████| 1500/1500 [00:53<00:00, 28.15it/s]\n",
      "Processing G: 100%|██████████| 1500/1500 [00:53<00:00, 28.00it/s]\n",
      "Processing H: 100%|██████████| 1500/1500 [00:53<00:00, 28.16it/s]\n",
      "Processing I: 100%|██████████| 1500/1500 [00:53<00:00, 28.23it/s]\n",
      "Processing J: 100%|██████████| 1500/1500 [00:53<00:00, 28.14it/s]\n",
      "Processing K: 100%|██████████| 1500/1500 [00:53<00:00, 28.22it/s]\n",
      "Processing L: 100%|██████████| 1500/1500 [00:53<00:00, 28.15it/s]\n",
      "Processing M: 100%|██████████| 1500/1500 [00:49<00:00, 30.55it/s]\n",
      "Processing N: 100%|██████████| 1500/1500 [00:56<00:00, 26.50it/s]\n",
      "Processing O: 100%|██████████| 1500/1500 [01:00<00:00, 24.97it/s]\n",
      "Processing P: 100%|██████████| 1500/1500 [00:54<00:00, 27.47it/s]\n",
      "Processing Q: 100%|██████████| 1500/1500 [00:59<00:00, 25.25it/s]\n",
      "Processing R: 100%|██████████| 1500/1500 [00:53<00:00, 28.19it/s]\n",
      "Processing S: 100%|██████████| 1500/1500 [00:48<00:00, 30.80it/s]\n",
      "Processing T: 100%|██████████| 1500/1500 [00:53<00:00, 28.18it/s]\n",
      "Processing U: 100%|██████████| 1500/1500 [05:20<00:00,  4.69it/s] \n",
      "Processing V: 100%|██████████| 1500/1500 [23:15<00:00,  1.07it/s] \n",
      "Processing W: 100%|██████████| 1500/1500 [00:55<00:00, 27.12it/s]\n",
      "Processing X: 100%|██████████| 1500/1500 [00:53<00:00, 28.14it/s]\n",
      "Processing Y: 100%|██████████| 1500/1500 [00:48<00:00, 31.10it/s]\n",
      "Processing Z: 100%|██████████| 1500/1500 [00:53<00:00, 28.25it/s]\n",
      "Processing _: 100%|██████████| 1500/1500 [00:52<00:00, 28.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "X, y = [], []\n",
    "label_map = {action: i for i, action in enumerate(actions)}\n",
    "\n",
    "for action in actions:\n",
    "    action_path = os.path.join(DATASET_PATH, action)\n",
    "    image_files = sorted(os.listdir(action_path))  # Limit to 1500 images per class\n",
    "    \n",
    "    for img_file in tqdm(image_files, desc=f\"Processing {action}\"):\n",
    "        img_path = os.path.join(action_path, img_file)\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        if image is None:\n",
    "            continue  # Skip corrupted files\n",
    "\n",
    "        keypoints = extract_keypoints(image)\n",
    "        X.append(keypoints)\n",
    "        y.append(label_map[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data extraction complete! Saved as gesture_data.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Save dataset\n",
    "with open(\"gesture_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump((X, y), f)\n",
    "\n",
    "print(\"✅ Data extraction complete! Saved as gesture_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55500, 132)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gesture sequences created! Shape: (55470, 30, 132)\n",
      "Training samples: 44376, Testing samples: 11094\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load extracted keypoints\n",
    "with open(\"gesture_data.pkl\", \"rb\") as f:\n",
    "    X, y = pickle.load(f)\n",
    "\n",
    "sequence_length = 30  # Number of frames per sequence\n",
    "\n",
    "# Reshape dataset into sequences\n",
    "X_sequences, y_sequences = [], []\n",
    "\n",
    "for i in range(len(X) - sequence_length):\n",
    "    X_sequences.append(X[i:i+sequence_length])  # Get 30-frame sequence\n",
    "    y_sequences.append(y[i+sequence_length-1])  # Label is the last frame's class\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Split into train/test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save processed sequences\n",
    "with open(\"gesture_sequences.pkl\", \"wb\") as f:\n",
    "    pickle.dump((X_train, X_test, y_train, y_test), f)\n",
    "\n",
    "print(f\"✅ Gesture sequences created! Shape: {X_sequences.shape}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shapes - X: (1110, 30, 225), y: (1110,)\n"
     ]
    }
   ],
   "source": [
    "# Load the arrays\n",
    "X = np.load('X_sequences.npy', allow_pickle=True)\n",
    "y= np.load('y_labels.npy', allow_pickle=True)\n",
    "\n",
    "print(f\"Loaded data shapes - X: {X.shape}, y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAHIL GUPTA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(30, 225)),\n",
    "    LSTM(128),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(37, activation='softmax')  # 37 output classes\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.0913 - loss: 3.5022 - val_accuracy: 0.1982 - val_loss: 2.9066\n",
      "Epoch 2/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.3092 - loss: 2.6471 - val_accuracy: 0.4685 - val_loss: 2.0791\n",
      "Epoch 3/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4744 - loss: 1.9138 - val_accuracy: 0.4730 - val_loss: 1.8431\n",
      "Epoch 4/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5618 - loss: 1.5561 - val_accuracy: 0.5315 - val_loss: 1.5764\n",
      "Epoch 5/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5983 - loss: 1.3949 - val_accuracy: 0.5676 - val_loss: 1.3943\n",
      "Epoch 6/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6720 - loss: 1.1058 - val_accuracy: 0.6712 - val_loss: 1.3229\n",
      "Epoch 7/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6845 - loss: 1.0599 - val_accuracy: 0.6757 - val_loss: 1.1093\n",
      "Epoch 8/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7776 - loss: 0.8027 - val_accuracy: 0.6667 - val_loss: 1.0666\n",
      "Epoch 9/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7394 - loss: 0.8105 - val_accuracy: 0.6216 - val_loss: 1.1198\n",
      "Epoch 10/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7357 - loss: 0.7758 - val_accuracy: 0.7027 - val_loss: 1.1053\n",
      "Epoch 11/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7503 - loss: 0.7282 - val_accuracy: 0.6757 - val_loss: 1.0563\n",
      "Epoch 12/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8201 - loss: 0.6177 - val_accuracy: 0.7297 - val_loss: 0.9708\n",
      "Epoch 13/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8312 - loss: 0.5316 - val_accuracy: 0.7252 - val_loss: 0.8274\n",
      "Epoch 14/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8154 - loss: 0.5342 - val_accuracy: 0.6937 - val_loss: 0.9354\n",
      "Epoch 15/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8016 - loss: 0.5446 - val_accuracy: 0.6802 - val_loss: 1.0498\n",
      "Epoch 16/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8386 - loss: 0.5076 - val_accuracy: 0.6937 - val_loss: 0.9410\n",
      "Epoch 17/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8105 - loss: 0.5498 - val_accuracy: 0.7523 - val_loss: 0.8067\n",
      "Epoch 18/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8131 - loss: 0.5615 - val_accuracy: 0.7117 - val_loss: 1.0389\n",
      "Epoch 19/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8091 - loss: 0.5539 - val_accuracy: 0.6847 - val_loss: 0.9967\n",
      "Epoch 20/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8317 - loss: 0.5066 - val_accuracy: 0.7432 - val_loss: 0.7782\n",
      "Epoch 21/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8665 - loss: 0.3845 - val_accuracy: 0.7432 - val_loss: 0.8605\n",
      "Epoch 22/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8934 - loss: 0.3315 - val_accuracy: 0.7613 - val_loss: 0.7833\n",
      "Epoch 23/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8683 - loss: 0.3666 - val_accuracy: 0.7432 - val_loss: 0.8280\n",
      "Epoch 24/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8758 - loss: 0.3381 - val_accuracy: 0.7838 - val_loss: 0.7753\n",
      "Epoch 25/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8959 - loss: 0.3279 - val_accuracy: 0.7658 - val_loss: 0.7291\n",
      "Epoch 26/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8936 - loss: 0.2858 - val_accuracy: 0.7703 - val_loss: 0.8478\n",
      "Epoch 27/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8606 - loss: 0.3685 - val_accuracy: 0.7748 - val_loss: 0.8290\n",
      "Epoch 28/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8715 - loss: 0.3650 - val_accuracy: 0.7342 - val_loss: 0.8903\n",
      "Epoch 29/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8460 - loss: 0.4208 - val_accuracy: 0.7297 - val_loss: 0.8583\n",
      "Epoch 30/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8592 - loss: 0.3602 - val_accuracy: 0.7477 - val_loss: 0.8590\n",
      "Epoch 31/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8903 - loss: 0.3134 - val_accuracy: 0.7748 - val_loss: 0.8551\n",
      "Epoch 32/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9296 - loss: 0.2393 - val_accuracy: 0.7568 - val_loss: 0.8167\n",
      "Epoch 33/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8985 - loss: 0.2781 - val_accuracy: 0.7523 - val_loss: 0.9728\n",
      "Epoch 34/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9086 - loss: 0.2789 - val_accuracy: 0.7748 - val_loss: 0.7655\n",
      "Epoch 35/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8865 - loss: 0.3252 - val_accuracy: 0.7523 - val_loss: 0.9351\n",
      "Epoch 36/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8433 - loss: 0.4777 - val_accuracy: 0.7072 - val_loss: 0.9940\n",
      "Epoch 37/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8471 - loss: 0.4299 - val_accuracy: 0.7072 - val_loss: 0.9983\n",
      "Epoch 38/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8131 - loss: 0.5255 - val_accuracy: 0.7162 - val_loss: 1.0454\n",
      "Epoch 39/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8874 - loss: 0.3411 - val_accuracy: 0.7703 - val_loss: 0.8424\n",
      "Epoch 40/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8942 - loss: 0.2900 - val_accuracy: 0.8018 - val_loss: 0.7705\n",
      "Epoch 41/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9227 - loss: 0.2441 - val_accuracy: 0.8018 - val_loss: 0.7913\n",
      "Epoch 42/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9231 - loss: 0.2281 - val_accuracy: 0.8198 - val_loss: 0.7714\n",
      "Epoch 43/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9224 - loss: 0.2240 - val_accuracy: 0.7973 - val_loss: 0.7757\n",
      "Epoch 44/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9108 - loss: 0.2375 - val_accuracy: 0.8198 - val_loss: 0.7609\n",
      "Epoch 45/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9097 - loss: 0.2303 - val_accuracy: 0.7793 - val_loss: 0.7902\n",
      "Epoch 46/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9255 - loss: 0.1903 - val_accuracy: 0.8378 - val_loss: 0.7623\n",
      "Epoch 47/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9298 - loss: 0.1813 - val_accuracy: 0.8063 - val_loss: 0.7781\n",
      "Epoch 48/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9044 - loss: 0.2433 - val_accuracy: 0.8018 - val_loss: 0.7820\n",
      "Epoch 49/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9085 - loss: 0.2104 - val_accuracy: 0.8198 - val_loss: 0.8098\n",
      "Epoch 50/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9189 - loss: 0.2184 - val_accuracy: 0.8198 - val_loss: 0.7987\n",
      "Epoch 51/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9282 - loss: 0.1966 - val_accuracy: 0.7928 - val_loss: 0.8155\n",
      "Epoch 52/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9082 - loss: 0.2246 - val_accuracy: 0.8198 - val_loss: 0.8144\n",
      "Epoch 53/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9227 - loss: 0.2153 - val_accuracy: 0.7928 - val_loss: 0.8308\n",
      "Epoch 54/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9159 - loss: 0.2224 - val_accuracy: 0.8153 - val_loss: 0.8193\n",
      "Epoch 55/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9022 - loss: 0.2392 - val_accuracy: 0.8198 - val_loss: 0.7795\n",
      "Epoch 56/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9297 - loss: 0.1786 - val_accuracy: 0.7883 - val_loss: 0.7954\n",
      "Epoch 57/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9292 - loss: 0.1742 - val_accuracy: 0.8198 - val_loss: 0.7970\n",
      "Epoch 58/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9059 - loss: 0.2274 - val_accuracy: 0.7928 - val_loss: 0.7990\n",
      "Epoch 59/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9206 - loss: 0.2061 - val_accuracy: 0.7883 - val_loss: 0.8130\n",
      "Epoch 60/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9254 - loss: 0.2010 - val_accuracy: 0.7883 - val_loss: 0.8408\n",
      "Epoch 61/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9248 - loss: 0.1805 - val_accuracy: 0.8153 - val_loss: 0.8408\n",
      "Epoch 62/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9229 - loss: 0.2211 - val_accuracy: 0.7883 - val_loss: 0.8123\n",
      "Epoch 63/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9143 - loss: 0.2119 - val_accuracy: 0.7838 - val_loss: 0.8488\n",
      "Epoch 64/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9360 - loss: 0.1786 - val_accuracy: 0.8243 - val_loss: 0.8007\n",
      "Epoch 65/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9254 - loss: 0.1979 - val_accuracy: 0.7928 - val_loss: 0.8194\n",
      "Epoch 66/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9332 - loss: 0.1889 - val_accuracy: 0.8243 - val_loss: 0.8087\n",
      "Epoch 67/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9163 - loss: 0.1994 - val_accuracy: 0.8153 - val_loss: 0.8129\n",
      "Epoch 68/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9395 - loss: 0.1536 - val_accuracy: 0.8153 - val_loss: 0.8153\n",
      "Epoch 69/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9403 - loss: 0.1596 - val_accuracy: 0.8153 - val_loss: 0.8376\n",
      "Epoch 70/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9095 - loss: 0.2426 - val_accuracy: 0.8243 - val_loss: 0.8235\n",
      "Epoch 71/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9237 - loss: 0.2159 - val_accuracy: 0.8243 - val_loss: 0.8110\n",
      "Epoch 72/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9301 - loss: 0.1905 - val_accuracy: 0.8243 - val_loss: 0.8560\n",
      "Epoch 73/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9175 - loss: 0.2141 - val_accuracy: 0.7748 - val_loss: 1.0491\n",
      "Epoch 74/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8513 - loss: 0.4463 - val_accuracy: 0.6622 - val_loss: 1.3171\n",
      "Epoch 75/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7053 - loss: 0.9221 - val_accuracy: 0.7207 - val_loss: 0.9787\n",
      "Epoch 76/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8199 - loss: 0.5182 - val_accuracy: 0.7568 - val_loss: 0.9013\n",
      "Epoch 77/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8371 - loss: 0.4083 - val_accuracy: 0.7703 - val_loss: 0.8572\n",
      "Epoch 78/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9047 - loss: 0.2820 - val_accuracy: 0.7883 - val_loss: 0.8298\n",
      "Epoch 79/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8909 - loss: 0.3355 - val_accuracy: 0.7613 - val_loss: 0.9679\n",
      "Epoch 80/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8477 - loss: 0.4215 - val_accuracy: 0.7613 - val_loss: 0.8547\n",
      "Epoch 81/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8335 - loss: 0.4197 - val_accuracy: 0.7117 - val_loss: 1.1130\n",
      "Epoch 82/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8751 - loss: 0.3717 - val_accuracy: 0.7613 - val_loss: 0.8378\n",
      "Epoch 83/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8862 - loss: 0.3025 - val_accuracy: 0.7883 - val_loss: 0.8413\n",
      "Epoch 84/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8984 - loss: 0.2765 - val_accuracy: 0.8198 - val_loss: 0.7396\n",
      "Epoch 85/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9121 - loss: 0.2442 - val_accuracy: 0.7838 - val_loss: 0.6659\n",
      "Epoch 86/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9154 - loss: 0.2194 - val_accuracy: 0.8018 - val_loss: 0.7966\n",
      "Epoch 87/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9194 - loss: 0.2016 - val_accuracy: 0.8153 - val_loss: 0.7357\n",
      "Epoch 88/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9143 - loss: 0.2277 - val_accuracy: 0.8108 - val_loss: 0.7086\n",
      "Epoch 89/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9333 - loss: 0.1793 - val_accuracy: 0.8018 - val_loss: 0.7585\n",
      "Epoch 90/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9068 - loss: 0.2299 - val_accuracy: 0.8468 - val_loss: 0.6238\n",
      "Epoch 91/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9374 - loss: 0.1796 - val_accuracy: 0.8468 - val_loss: 0.6556\n",
      "Epoch 92/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9251 - loss: 0.1935 - val_accuracy: 0.8108 - val_loss: 0.6939\n",
      "Epoch 93/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9396 - loss: 0.1587 - val_accuracy: 0.8108 - val_loss: 0.7150\n",
      "Epoch 94/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9251 - loss: 0.1963 - val_accuracy: 0.8288 - val_loss: 0.7234\n",
      "Epoch 95/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9272 - loss: 0.2131 - val_accuracy: 0.8378 - val_loss: 0.6910\n",
      "Epoch 96/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9223 - loss: 0.2149 - val_accuracy: 0.8423 - val_loss: 0.6906\n",
      "Epoch 97/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9266 - loss: 0.1794 - val_accuracy: 0.8423 - val_loss: 0.6836\n",
      "Epoch 98/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9326 - loss: 0.1806 - val_accuracy: 0.8468 - val_loss: 0.6855\n",
      "Epoch 99/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9252 - loss: 0.2021 - val_accuracy: 0.8468 - val_loss: 0.6873\n",
      "Epoch 100/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9271 - loss: 0.1903 - val_accuracy: 0.8378 - val_loss: 0.6913\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=100,\n",
    "                    batch_size=32,)\n",
    "model.save('my_model2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_weights('my_model2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Usage example\u001b[39;00m\n\u001b[32m     52\u001b[39m image_path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mc:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mVS code\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mai\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m4.jpg\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your image path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m gesture, confidence = \u001b[43mpredict_gesture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted Gesture: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgesture\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with confidence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mpredict_gesture\u001b[39m\u001b[34m(image_path, model, actions)\u001b[39m\n\u001b[32m     19\u001b[39m keypoints = np.expand_dims(keypoints, axis=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# Shape becomes (1, 1, 225)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m res = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m(keypoints)[\u001b[32m0\u001b[39m]\n\u001b[32m     23\u001b[39m predicted_idx = np.argmax(res)\n\u001b[32m     24\u001b[39m predicted_gesture = actions[predicted_idx]\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "def predict_gesture(image_path, model, actions):\n",
    "    \"\"\"Predict gesture from a single image\"\"\"\n",
    "    # Read and process image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    ) as holistic:\n",
    "        # Make detection\n",
    "        results = holistic.process(image_rgb)\n",
    "        \n",
    "        # Extract keypoints\n",
    "        keypoints = extract_keypoints(results)\n",
    "        \n",
    "        # Reshape for model (1 sequence with 1 frame)\n",
    "        keypoints = np.expand_dims(keypoints, axis=0)  # Shape becomes (1, 225)\n",
    "        keypoints = np.expand_dims(keypoints, axis=0)  # Shape becomes (1, 1, 225)\n",
    "        \n",
    "        # Make prediction\n",
    "        res = model.predict(keypoints)[0]\n",
    "        predicted_idx = np.argmax(res)\n",
    "        predicted_gesture = actions[predicted_idx]\n",
    "        confidence = res[predicted_idx]\n",
    "        \n",
    "        # Visualize results\n",
    "        annotated_image = image.copy()\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        if results.left_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        if results.right_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        \n",
    "        # Add prediction text\n",
    "        cv2.putText(annotated_image, f\"{predicted_gesture} ({confidence:.2f})\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display results\n",
    "        cv2.imshow(\"Prediction\", annotated_image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        return predicted_gesture, confidence\n",
    "\n",
    "# Usage example\n",
    "image_path = r\"c:\\VS code\\ai\\4.jpg\"  # Replace with your image path\n",
    "gesture, confidence = predict_gesture(image_path, model, actions)\n",
    "print(f\"Predicted Gesture: {gesture} with confidence {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Initialize MediaPipe drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    \"\"\"Draw all detected landmarks on the image\"\"\"\n",
    "    # Draw pose connections\n",
    "    # mp_drawing.draw_landmarks(\n",
    "    #     image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "    #     landmark_drawing_spec=mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "    #     connection_drawing_spec=mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "    \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "        connection_drawing_spec=mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "    \n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "        connection_drawing_spec=mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "    \n",
    "    return image\n",
    "\n",
    "def visualize_prediction(image, results, prediction, confidence):\n",
    "    \"\"\"Combine landmarks and prediction visualization\"\"\"\n",
    "    # Draw landmarks first\n",
    "    annotated_image = draw_landmarks(image.copy(), results)\n",
    "    \n",
    "    # Add prediction text\n",
    "    cv2.putText(annotated_image, f\"{prediction} ({confidence:.2f})\", \n",
    "               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # Add confidence bar\n",
    "    bar_width = int(confidence * 200)\n",
    "    cv2.rectangle(annotated_image, (10, 40), (10 + bar_width, 60), (0, 255, 0), -1)\n",
    "    \n",
    "    return annotated_image\n",
    "\n",
    "# Real-time prediction with visualization\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "prediction_history = []\n",
    "\n",
    "with mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Make detection\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        # Extract keypoints\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]  # Maintain sequence length\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            # Make prediction\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            prediction = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "            \n",
    "            # Update prediction history for smoothing\n",
    "            prediction_history.append(prediction)\n",
    "            if len(prediction_history) > 5:\n",
    "                prediction_history.pop(0)\n",
    "            \n",
    "            # Get most frequent recent prediction\n",
    "            final_prediction = max(set(prediction_history), key=prediction_history.count)\n",
    "            \n",
    "            # Visualize\n",
    "            frame = visualize_prediction(frame, results, final_prediction, confidence)\n",
    "        \n",
    "        # Display\n",
    "        cv2.imshow('Gesture Recognition', frame)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
